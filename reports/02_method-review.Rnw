\section{Brief Review of Existing Methods for Variable Selection \label{ExMethods}}
As is evident in Table \ref{tab:vSelect}, there are a number of ways to complete variable selection. Before describing several of these methods, it is important to consider the motivation for engaging in variable selection. Consider the following scenario, common in public health research. A dataset contains measurements on $n$ different individuals and $P$ total covariates per individual. One of the most common objectives is then to explore the relationship of the explanatory covariates $X$ with the outcome $Y$, statistically expressed as $E[Y|X]$. As discussed earlier, this relationship is most commonly explored by the use of a linear regression of the observed outcomes $Y_n \in \mathbb{R}^{n\times 1}$ on the observed covariates $X_n \in \mathbb{R}^{n\times P}$ by estimating the coefficients $\beta$. 

%Given a sample size of $n$ observations, one could hypothetically build a saturated linear regression model with an intercept and $n-1$ additional coefficients. While this saturated model fits the observed data perfectly (and therefore has no bias), modeling is not done this way for a number of reasons including the bias-variance tradeoff and the interpretability of the coefficients. 

One of the criteria commonly used for selecting the ``best'' estimator of a parameter is the mean squared error (MSE) of the estimator. Expressed in Equation \ref{MSE}, where, in the context of a linear regression, $\hat{\beta}$ is the estimator and $\beta$ is the true parameter value, the MSE relies on the bias and variance of the estimator $\hat{\beta}$. In order to minimize the MSE, one then tries to find estimators that have minimal bias and minimal variance. 

\begin{equation} \label{MSE}
MSE_{\hat{\beta}} = E_{\beta}[(\hat{\beta} - \beta)^2] = Var_{\beta}(\hat{\beta}) + (Bias_{\beta}(\hat{\beta}))^2
\end{equation}

Practically speaking, this bias-variance tradeoff is the constant battle of finding estimators that approximate the truth well, and as such fit the data well, but are also very stable. Stability can be visualized as the smoothness of a regression line. For example, if an outcome $Y$ is modeled as $E[Y|X] = a$, where $a$ is the mean of $Y$, then for any $X = x$, the estimate of $Y$ will always be its marginal mean. This is a very stable estimator: it does not change at all regardless of the size of change in $X = x$. It is also highly biased if $E[Y|X]$ does depend on the realization of $X$. In this example, the tradeoff has been made to find an estimator that has low variance but the potential for high bias. If a model makes use of too many covariates, the estimator may be less biased, but highly variable. Further, the more terms included in a linear regression model the greater the concern of overfitting, or modeling noise that is specific to the dataset at hand rather than the true underlying signal. More moderate tradeoffs such as a small increase in bias can lead to a large decrease in variance, resulting in a smaller MSE overall. This is the case with LASSO, which will be discussed further in Section \ref{lasso}.   

Linear models run into problems when $P$ is much larger than $n$, when $P$ is equal to $n$, or when $P$ is slightly less than $n$. In each of these settings, researchers may be interested in selecting an ``optimal'' subset of covariates $p \in P$ to use in the linear regression instead of using the entire original set of $P$ covariates. With respect to intepretability, models that make use of fewer variables are typically easier to understand and the variables are conceptually simpler to map with respect to each other than a large number of variables with complicated relationships. When researchers have \textit{a priori} knowledge as to the potential relationships of particular variables (e.g. an exposure of interest and well-defined potential confounders) a regression model can be used to test these \textit{a priori} hypotheses. Regression with fewer variables can also be desirable for more exploratory analyses, as once again, interpretability is generally a primary objective.  %Bias and variance implications with respect to model size will be discussed in Section \ref{lasso}. The goals of accurately representing the complexity of the world while simultaneously finding low-variance estimators and interpretable models are directly at odds with each other.

To add to the difficulty of including enough covariates so as to accurately represent the complexity of the world while simultaneously finding low variance estimators, a great number of methods and criteria exist for selecting an ``optimal'' subset of covariates. While efforts have been made to summarize the existing literature into a series of best practices (Sauer et al. 2013), consistent application or even agreement on these practices have yet to have been adopted by the field at large. In the following paragraphs, I attempt to briefly catalogue the most common statistical techniques for variable selection in the context of linear regression models as well as provide an overview of their limitations. 

\subsection{Subset and Stepwise Selection}

The \textit{best subset} method aims to determine the optimal subset of covariates by an exhaustive search through the $P$ possible variables. In this setting, every possible subset of variables, of which there are $2^P$, is tested and optimality is determined by a global certain criteria (Christensen 2011, pp. 381-385). A variant of the method minimizes the sum of squared errors by testing every possible subset of variables of a predetermined subset size $p \in P$. In either setting, this is a computationally expensive method in that every variable is included and excluded in every possible combination. Further, there are considerable multiple testing issues that arise, worsened by the implementation of criteria that was originally meant to test hypotheses of effect size not model selection (Dziak et al. 2005).  

\textit{Stepwise selection}, typically via forward or backward steps, significantly shrinks the number of possible subsets one must consider in order to find the optimal subset and essentially functions as an approximation of the \textit{best subset} method. For \textit{forward selection}, the intercept is first fit. In a sense, this is the first subset and comparing any other model against this first subset investigates whether an additional variable makes a significant contribution to the fit of the model. A few of the ways in which contributions can be considered significant includes observed increases in the model $R^2$ via an $F$ statistic comparing the sum of squared errors from the smaller subset with those from the larger subset, via $T$ tests regarding whether the additional coefficients on a larger model are significantly different from the null, or by the largest increase in absolute partial correlation (Christensen 2011, pp. 385). This process is continued, adding one variable at a time to the model (always that which helps to best improve on the decided criteria) until adding an additional variable does not significantly improve the fit. \textit{Backward selection} follows the same path, but starting from a full model with all covariates included and removing covariates until a particular stopping criterion has been met. 

These stepwise methods improve on \textit{best subset} selection by decreasing the number of possible subsets, but still fall prey to faults in stopping criteria and multiple testing, among other things. Many stopping critera focus primarily on local performance rather than global performance. Hence, local stopping criteria may lead an individual to limit variables to a locally most efficient though not globally most efficient model. Local stopping criteria also have difficulty when detecting small but important improvements and can prematurely stop model building when the gains to adding additional coefficients plateaus prematurely. Finally, while the gains in computational efficiency were at one time appealing, new software have made the reliance on step-wise variable selection to find a decent model, though almost certainly sub-optimal, unnecessary and undesirable. The faults of this method have been well documented in text books (Hastie et al. 2008; Christensen 2011) and research papers alike (Greenland 2008).    

\subsection{Effect Estimate Change} \label{EEChange} 
\textit{Effect estimate change} has proven via simulation studies to be quite effective at identifying confounders, which are often the variables one hopes to control for in models exploring the effect of a particular exposure or set of exposures. Consider the model in Equation \ref{EEmod}, where $Y$ is the outcome of interest, $X$ the exposure(s) of interest and $W$ the potential confounder(s). The general premise is that a confounder $W$ obscures the relationship between the exposure of interest, $X$, and the outcome, $Y$. 

\begin{equation}\label{EEmod}
E[Y|X, W] = \alpha + \beta X + \gamma W 
\end{equation}

For the change in estimate criteria, one first fits the unadjusted model, Equation \ref{unadj}, which does not include the confounders $W$. Then after fitting the adjusted model, Equation \ref{EEmod}, a comparison of the coefficient estimate from the unadjusted model $\hat{\kappa}$ is compared to the coefficient estimate $\hat{\beta}$ from the adjusted model. 

\begin{equation}\label{unadj}
E[Y|X, W] = \alpha + \kappa X 
\end{equation}


Comparing the effect estimate, $\hat{\kappa}$, in a model without control for the hypothetical confounder to that which includes the hypothetical confounder$\hat{\beta}$, a 10\% change in the estimated effect as measured by $(\hat{\kappa} - \hat{\beta})/\hat{\kappa}$, suggests the variable $W$ is a confounder.

This premise, while attractive, presents another set of concerns. First, the subset of variables considered as potential confounders must be identified. Recommendations have been made to follow the causal mapping formalized by Robins in 1987. Once potential confounders have been identified, one must be wary of controlling for too many confounders when the sample size is not infinite. Regardless of the confounders' theoretical importance, including too many confounders can result in overstratification of the data, unwieldy sparsity in the covariate distribution and instability in effect estimates. Second, there must also be in place \textit{a priori} criteria as to how large of a change in effect must be observed to consider the added variable to be a confounder (Robins and Greenland 1986). Historically, a 10\% cutoff has been observed, whereby effect estimates that change by 10\% upon the inclusion or removal of a hypothetical confounder are said to display evidence of confounding. However, a recent paper published in the \textit{Journal of Epidemiology} demonstrated that 10\% may not always be appropriate. In this particular paper, via simulation and utilization of the NHANES dataset, Lee demonstrated that it would be better to first examine the change in estimate for the standardized exposure and standardized outcome with the inclusion of a random variable simulated from a standard normal distribution. Then, variables that induce a change in estimate greater than the 95th percentile of the previous model including the standard random variable would be considered confounders (Lee 2014). In a sense, this compares the observed change in estimate between the exposure and confounder of interest with the estimated change in estimate associated with the inclusion of a completely independent random variable. While reasonably simple, the 10\% cutoff still seems to prevail in research without the use of suggested corrective processes. 

\subsection{Propensity Scores}

\textit{Propensity score methods}, proposed by Rosenbaum and Rubin (1983), aim for similar goals as those in Subsection \ref{EEChange}. Used to control for confounders in non-experimental studies where there is an exposure (or set of exposures) of interest $X$, \textit{propensity score methods} examine the probability of a certain exposure $X$ given a set of variables $W$. This is often completed via a logistic model, as in Equation \ref{propScore}, but can be done non-parametrically as well. 

\begin{equation}\label{propScore}
g(W) = Pr(X = 1 | W) = \frac{e^{\alpha + \beta W}}{1 + e^{\alpha + \beta W}}
\end{equation}

Unlike Equation \ref{EEmod}, where $W$ was assumed to be a set of confounders, the propensity score method considers a set of baseline covariates $W$ and assumes that controlling for propensity score considering $W$ is sufficient to break most confounding. This is implemented in Equation \ref{propScore2}. Mathematically, this is an independence assumption: assume an individual's exposure, once their background has been controlled for, arose independently of their future outcome, $Y \perp X | g(W)$  Mimicking randomized control trials, observations with similar propensity scores are compared to each other in order to understand the relationship of the exposure $X$ and outcome $Y$ in the assumed absense of external confounding (Rosenbaum and Rubin 1983). 

\begin{equation}\label{propScore2}
E[Y|X,W] = \alpha + \beta X + \gamma g(W)
\end{equation}

This technique has proven useful when the outcome of interest is rare, the exposure is common and other methods of variable selection would lead to unmanageable sparsity. However, when the outcome is not rare and other methods of variable selection may be appropriate, it can be difficult to determine the benefits of continued use of controlling for propensity scores. A 2006 paper reviewed the growing use of \textit{propensity score methods} in these settings and found no empirical evidence of improvements in performance when compared to other appropriate confounder identification and control methods (St{\"u}rmer 2006). 

%\subsection{Principal Components}

%The method of principal components aims separate the the structure in the data from the noise. The structure is summarized by a model of principal components, where the principal components can be imagined as a new coordinate system for the original data. To project the observed data onto this new coordinate system requires a mapping from each observed variable to the location of the new variable. This is accomplished in principal components analysis (PCA) by  

