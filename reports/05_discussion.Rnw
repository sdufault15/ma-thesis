\section{Discussion}

The application in this paper demonstrated a simple three step process to complete LASSO estimation. First, it is necessary to determine what will be considered by the model. In this application, and in other non-experimental studies like it, this selection is one point at which existing literature, prior knowledge and statistical tools such as DAGs can be incredibly informative. A concern with statistical methods designed for high dimensional data is that the method is simply a ``black-box''. By being explicit in the the theories and existing understanding which is used to inform which variables or types of variables are considered for selection in the model, this ``black-box'' concern can begin to be satisfied. This is a highly recommended practice even for traditional analyses, particularly in non-experimental studies such as those generated from large pre-existing databases (Sauer, Brookhart and Roy 2013). Second, cross-validation can be used to determine an optimal penalty. Cross-validation ascertains that the data used to build the model is different than the data used to test the model's performance. This prevents the application of a penalty that is strictly optimal to the current data itself, i.e. avoiding overfitting. Third, LASSO regression is completed and coefficient estimates can be returned using the optimal penalty or another useful penalty such as those that induce greater sparsity, such as the strictest penalty which returns a CVM within one standard error of the minimum. As seen in Figure \ref{fig:rss}, least squares regression tends to fit the data well, but tends to produce highly variable estimates when faced with new data. The shrinkage LASSO enforces, while creating bias in the coefficient estimates themselves, produces less variable predictions when faced with a new dataset, therefore minimizing the overall MSE. In three lines of code, it is possible to produce a model that, contrary to least squares, 1) is more rigorous to collinearity, 2) can select a subset of covariates even when there are more covariates than observations ($p >> n$), and 3) produces less variable estimates.

A next step would be to consider the blossoming field of high dimensional and post model selection inference and how this pertains to LASSO coefficient estimates. In the most traditional regression analyses, the subset of covariates considered by a model are specified \textit{a priori}. Data is then collected pertaining to these variables and a model is fit. In data-adaptive methods, including step-wise, the covariates included in the model are chosen based on the data. As such, the inference regarding their coefficient estimates should reflect the fact that they have already been ``tested'' in a way that determined whether or not the variable remained in the model set. It is this difference, a prespecified set of covariates versus an adaptive set of covariates, that makes traditional applications of inference incorrect and misleading. As such, a number of methods have been suggested for use with LASSO regression, though a consensus has not been met as to which method may be most favorable. \textit{Statistical Learning with Sparse Data} (Hastie et al. 2015) describes several such methods ranging from Bayesian models to bootstrapping. As of the writing of this paper, a method for obtaining p-values within the \texttt{glmnet} package was not yet implemented. 

In conclusion, variable selection is an important part of completing reliable estimation. As such, journals should require greater transparency in order to advance the field as a whole. %Concerns that focusing on these processes may distract from the key findings of the research should not drive this conversation. 
Many frequently used methods are highly contested for well-documented reasons. Methods that were once optimal because of properties such as low computational expense or the sense of stability that comes from completing an ordered set of stepwise tests are losing relevance to newer methods that make better use of modern computing power, respond better to the demands of high dimensional datasets and help remove the researcher bias of cherry-picking model results that fit the research agenda. LASSO is simply one of many methods that aims to intelligently combine methods, such as least squares, that are well known and understood with the need for high dimensional variable selection. In order to perform lasting research and avoid the growing concerns of null findings (Ioannadis 2005), public health research should continue to pursue greater transparency and the implementation of improved methods. 
